{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rake_nltk import Rake\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting keyframes.....\n",
      "generating captions.....\n",
      "generating keywords.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['a bunch of flowers on a table',\n",
       "  'a bunch of flowers in a vase',\n",
       "  'a bouquet of flowers on a table',\n",
       "  'beautiful flowers in vase on table',\n",
       "  'a bouquet of flowers with a blur effect',\n",
       "  'a bouquet of flowers on a table'],\n",
       " ['flowers',\n",
       "  'table',\n",
       "  'bouquet',\n",
       "  'bunch',\n",
       "  'vase',\n",
       "  'beautiful',\n",
       "  'effect',\n",
       "  'blur'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_keyframes(video_path, output_dir, frame_interval):\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    keyframe_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            keyframe_path = os.path.join(output_dir, f\"keyframe_{keyframe_count}.jpg\")\n",
    "            cv2.imwrite(keyframe_path, frame)\n",
    "            keyframe_count += 1\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "def generate_captions(image_paths):\n",
    "    captions = []\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # unconditional image captioning\n",
    "        inputs = processor(image, return_tensors = \"pt\")\n",
    "        output = model.generate(**inputs, max_new_tokens = 30)\n",
    "        decoded_caption = processor.decode(output[0], skip_special_tokens = True)\n",
    "        captions.append(decoded_caption)\n",
    "\n",
    "    return captions\n",
    "\n",
    "def generate_keywords_with_llm(captions):\n",
    "    # concatenate all captions into a single string, separated by commas\n",
    "    caption_text = repr(captions)\n",
    "    print(caption_text)\n",
    "    # load pre-trained gpt2 model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    # load other larger pre-trained llm\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "    # create a Hugging Face pipeline for text generation\n",
    "    generator = pipeline(\"text-generation\", model = model, tokenizer = tokenizer, max_new_tokens = 150, do_sample = True, temperature = 0.9)\n",
    "\n",
    "    # wrap the pipeline in langchain's huggingfacepipeline\n",
    "    llm = HuggingFacePipeline(pipeline = generator)\n",
    "\n",
    "    # define a prompt template for extracting keywords\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"captions\"],\n",
    "        template=\"\"\"\n",
    "Given the following list of captions extracted from video keyframes, extract a set of meaningful and relevant keywords or short phrases from the captions. These keywords should represent the main objects, subjects, and actions described in the captions. The goal is to summarize the most important concepts in a concise list of keywords.\n",
    "\n",
    "Here are the captions:\n",
    "{captions}\n",
    "\n",
    "Extracted Keywords:\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    # create an llm chain using the huggingfacepipeline\n",
    "    llm_chain = LLMChain(prompt = prompt_template, llm = llm)\n",
    "\n",
    "    # run the chain to extract keywords\n",
    "    keywords = llm_chain.run({\"captions\": caption_text})\n",
    "\n",
    "    print(\"extracted keywords:\", keywords)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def extract_keywords_tfidf(captions):\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(captions)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.sum(axis = 0).A1\n",
    "    \n",
    "    sorted_indices = scores.argsort()[::-1]\n",
    "    sorted_keywords = [(feature_names[i], scores[i]) for i in sorted_indices]\n",
    "    \n",
    "    keywords = [keyword for keyword, score in sorted_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def extract_keywords_rake(captions):\n",
    "    rake = Rake()\n",
    "    text = \" \".join(captions)\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    ranked_phrases = rake.get_ranked_phrases_with_scores()\n",
    "    keywords = [phrase for score, phrase in ranked_phrases]\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def process_video(video_path, frame_interval = 30):\n",
    "    print(\"extracting keyframes.....\")\n",
    "    keyframes_dir = \"keyframes\"\n",
    "    keyframes = extract_keyframes(video_path, keyframes_dir, frame_interval)\n",
    "\n",
    "    print(\"generating captions.....\")\n",
    "    captions = generate_captions(keyframes)\n",
    "\n",
    "    print(\"generating keywords.....\")\n",
    "    # generate_keywords_with_llm(captions)\n",
    "    # other_keywords = extract_keywords_rake(captions)\n",
    "    keywords = extract_keywords_tfidf(captions)\n",
    "\n",
    "    return captions, keywords\n",
    "\n",
    "video_path = \"video/mayvortexx181100248.mp4\"\n",
    "captions, keywords = process_video(video_path)\n",
    "\n",
    "captions, keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
